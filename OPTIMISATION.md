# Optimisation Algorithms

Optimisation Algorithms are used to update weights and biases i,e. the internal parameters of a model to reduce the error. They can be divided into two categories:

### Constant Learning Rate Algorithms

Most widely used Optimisation Algorithm, the Stochastic Gradient Descent falls under this category.

### Adaptive Learning Algorithms

The challenge of using gradient descent is that their hyper parameters have to be defined in advance and they depend heavily on the type of model and problem. Another problem is that the same learning rate is applied to all parameter updates. If we have sparse data, we want to update the parameters in different extent instead.

Adaptive gradient descent algorithms such as Adagrad, Adadelta, RMScprop, Adam, provide an alternative to classical SGD. They have per-parameter learning rate methods, which provide heuristic approach without requiring expensive work in tuning hyperparameters for the learning rate schedule manually.

## Working with Optimisation Functions

### Stochastics Gradient Decent



### Referred from :
1. https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c

2. https://blog.algorithmia.com/introduction-to-loss-functions/

## Authors

* **Prashant Piprotar** - - [Prash+](https://github.com/prashplus)
and visit my blog [Nimbus](http://prashplus.blogspot.com) for more Tech Stuff
### http://www.prashplus.com